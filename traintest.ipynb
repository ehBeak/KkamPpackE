{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "traintest.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMtYc+0Qe+srUbZ5RGQwPa5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ehBeak/KkamPpackE/blob/main/traintest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hDuqM_Zm7uQ"
      },
      "source": [
        "mount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmBZSeJam7Ui",
        "outputId": "08d32d56-359a-4dcb-ecb8-f9a07aa27883"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XSwQC6Co4BZ",
        "outputId": "28974833-9ad0-410a-942a-8f8ff3b0efc1"
      },
      "source": [
        "%cd /content/drive/MyDrive/articulated-animation/articulated-animation-main/articulated-animation-main"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/articulated-animation/articulated-animation-main/articulated-animation-main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygBiP1l4nKny"
      },
      "source": [
        "기본 세팅"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTioGTqVnKEd",
        "outputId": "cc6290ff-7eb4-4d1c-dd79-1dba61d85431"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imageio==2.3.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (2.3.0)\n",
            "Requirement already satisfied: matplotlib==2.2.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (2.2.2)\n",
            "Requirement already satisfied: numpy==1.15.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: pandas==0.23.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (0.23.4)\n",
            "Requirement already satisfied: Pillow==5.2.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (5.2.0)\n",
            "Requirement already satisfied: PyYAML==5.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (5.1)\n",
            "Requirement already satisfied: scikit-image==0.14.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (0.14.0)\n",
            "Requirement already satisfied: scikit-learn==0.19.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (0.19.2)\n",
            "Requirement already satisfied: scipy==1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (1.1.0)\n",
            "Requirement already satisfied: torch==1.4.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (1.4.0)\n",
            "Requirement already satisfied: torchvision==0.2.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (0.2.1)\n",
            "Requirement already satisfied: tqdm==4.24.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (4.24.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.2.2->-r requirements.txt (line 2)) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.2.2->-r requirements.txt (line 2)) (0.10.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.2.2->-r requirements.txt (line 2)) (2018.9)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.2.2->-r requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.2.2->-r requirements.txt (line 2)) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.2.2->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: cloudpickle>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.14.0->-r requirements.txt (line 7)) (1.3.0)\n",
            "Requirement already satisfied: dask[array]>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.14.0->-r requirements.txt (line 7)) (2.12.0)\n",
            "Requirement already satisfied: networkx>=1.8 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.14.0->-r requirements.txt (line 7)) (2.6.3)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.14.0->-r requirements.txt (line 7)) (1.1.1)\n",
            "Requirement already satisfied: toolz>=0.7.3 in /usr/local/lib/python3.7/dist-packages (from dask[array]>=0.9.0->scikit-image==0.14.0->-r requirements.txt (line 7)) (0.11.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yl95sGX8mqBD"
      },
      "source": [
        "To train a model run:\n",
        "\n",
        "The code will create a folder in the log directory (each run will create a time-stamped new folder). Checkpoints will be saved to this folder. To check the loss values during training see log.txt. You can also check training data reconstructions in the train-vis subfolder. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMQ6aRnlmiSk"
      },
      "source": [
        "# dataset_name.yaml 정의 => params 설정하기\n",
        "!CUDA_VISIBLE_DEVICES=0 python run.py --config config/dataset_name.yaml --device_ids 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qJcu8Sim2Me"
      },
      "source": [
        "Then to train Animation via disentaglement (AVD) use:\n",
        "\n",
        "Where {folder} is the name of the folder created in the previous step. (Note: use backslash '' before space.) This will use the same folder where checkpoint was previously stored. It will create a new checkpoint containing all the previous models and the trained avd_network. You can monitor performance in log file and visualizations in train-vis folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3n3N1cUrYIP"
      },
      "source": [
        "# {folder} : 이전 단계에서 만들어진 폴더이름\n",
        "# https://github.com/AliaksandrSiarohin/pose-evaluation // pose에 관한 인식코드(그외 표정도 있음)\n",
        "!CUDA_VISIBLE_DEVICES=0 python run.py --checkpoint log/{folder}/cpk.pth --config config/dataset_name.yaml --device_ids 0 --mode train_avd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEPTDYJnsFJt"
      },
      "source": [
        "GPU: 1 V100\n",
        "dataset: 256x256\n",
        "trainning time : 2days\n",
        "\n",
        "\n",
        "[http://jaynewho.com/post/8]\n",
        "\n",
        "**Checkpoint** : 학습시킨 딥러닝 모델을 저장하는 방법 \n",
        "=> 학습된 모델의 variable 값 저장\n",
        "\n",
        "**tf.train.Saver()** : Checkpoint 파일을 저장해주는 클래스\n",
        "\n",
        "checkpoint\n",
        "train1.ckpt.data-00000-of-00001\n",
        "train1.ckpt.index\n",
        "train1.ckpt.meta\n",
        "\n",
        "**Checkpoint State Protocol Buffer** \n",
        "-> Saver의 saver 모듈을 이용해 모델을 저장할 때, Saver은 Checkpoint State Protocol Buffer를\n",
        "saved/checkpoint 파일에 담아 저장하고 새로운 job으로 학습할 때마다 업데이트 해 저장한다.\n",
        "\n",
        "1. model_checkpoint_path : 가장 최근에 저장된 job.ckpt파일의 path정보\n",
        "2. all_model_checkpoint_paths : 최근에 저장된 job_i.ckpt파일들의 path정보 list\n",
        "-> 보통 saved 폴더에서 가장 최신의 체크포인트 파일을 불러와 모델을 재학습시키거나 테스트 함\n",
        "-> 2의 가장 마지막 원소 == 1\n",
        "\n",
        "**Checkpoint State Protocol Buffer 이용법**\n",
        "1. tf.train.get_checkpoint_state(saved_dir_path)\n",
        "2. tf.train.latest_checkpoint(saved_dir_path)\n",
        "\n",
        "Checkpoint 불러오기\n",
        "\n",
        "\n",
        "**Trainning**\n",
        "\n",
        "CUDA_VISIBLE_DEVICES=0 python run.py --config config/dataset_name.yaml --device_ids 0\n",
        "\n",
        "log dir : Checkpoint 파일들이 저장됨\n",
        "log.txt : loss value확인가능\n",
        "train-vis subfolder : trainning data reconstrutions 확인가능\n",
        "\n",
        "**Animation via disentaglemnt(AVD)**\n",
        "\n",
        "CUDA_VISIBLE_DEVICES=0 python run.py --checkpoint log/{folder}/cpk.pth --config config/dataset_name.yaml --device_ids 0 --mode train_avd\n",
        "\n",
        "{folder} : 이전 단계에서 만들어졌던 폴더\n",
        "이전의 모델들이 들어가있는 체크포인트가 만들어짐\n",
        "\n",
        "**Evaluation on video reconstruction**\n",
        "CUDA_VISIBLE_DEVICES=0 python run.py --config config/dataset_name.yaml --mode reconstruction --checkpoint log/{folder}/cpk.pth\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ1iQVSQvxSw"
      },
      "source": [
        "# https://github.com/AliaksandrSiarohin/video-preprocessing\n",
        "# 사용할 영상 dataset 경로\n",
        "\n",
        "# requirements -> requirements 다른 경로임\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# Load youtube-dl:  id 본인이 결정\n",
        "!wget https://yt-dl.org/downloads/latest/youtube-dl -O youtube-dl\n",
        "\n",
        "!chmod a+rx youtube-dl\n",
        "\n",
        "#Run script to download videos,\n",
        "# there are 2 formats that can be used for storing videos one is .mp4 \n",
        "#and another is folder with .png images. \n",
        "#While .png images occupy significantly more space, \n",
        "#the format is loss-less and have better i/o performance when training.\n",
        "\n",
        "!python load_videos.py --metadata taichi-metadata.csv --format .mp4 --out_folder taichi --workers 8\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkPJA635wsmL"
      },
      "source": [
        "**Training on your own data**\n",
        "\n",
        "1. Resize all the videos to the same size, e.g 256x256, the videos can be in '.gif', '.mp4' or folder with images. We recommend the latter, for each video make a separate folder with all the frames in '.png' format. This format is loss-less, and it has better i/o performance.\n",
        "\n",
        "2. Create a folder data/dataset_name with 2 subfolders train and test, put training videos in the train and testing in the test.\n",
        "\n",
        "3. Create a config file config/dataset_name.yaml. See description of the parameters in the config/vox256.yaml. Specify the dataset root in dataset_params specify by setting root_dir: data/dataset_name. Adjust other parameters as desired, such as the number of epochs for example. Specify id_sampling: False if you do not want to use id_sampling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw_RWAG0w7HU"
      },
      "source": [
        "\n",
        "\n",
        "1.   비디오를 256x256 사이즈로 만들기(gif, mp4, images 가능)\n",
        "2.   각각의 비디오는 프레임당 .png포맷으로 만들것임 후자추천\n",
        "3. 후자가 더 좋은 결과 나온다.\n",
        "4. data/dataset_name 폴더 만들기\n",
        "5. data/dataset_name폴더는 2개의 서브폴더를 가짐 각각의 이름은 train과 test임\n",
        "6. training 비디오는 train폴더에 testing 비디오는 test폴더에 넣기\n",
        "7. **[config/dataset_yaml이라는 파일 만들기]**\n",
        "8. dataset_yaml파일에 들어갈 내용은 vox256.yaml을 참고하기\n",
        "9. dataset root를 지정하기 setting root_dir:data/dataset_name에 dataset_params를 지정하면 된다.\n",
        "10. 에폭이나 배치사이즈는 알아서 원하는대로 결정하기\n",
        "11. id_smapling원하지 않으면 id_smpling:False로 지정하기\n",
        "\n"
      ]
    }
  ]
}